---
layout: post
title: "결정 트리"
date: 2021-08-25
excerpt: "결정 트리 정리"
tags: [machine learning, data science]
comments: true
---

## 1. 분류의 개요
* **분류**: 학습 데이터로 주어진 데이터의 피처와 레이블값을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것
    * 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 니지한 뒤 새롭게 관측된 데이터에 대한 레이블을 판단하는 것
* 분류 알고리즘
    1. 베이즈 통계와 생성 모델에 기반한 나이브 베이즈
    2. 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀
    3. 데이터 균일도에 따른 규칙 기반의 결정 트리 
    4. 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신
    5. 근접 거리를 기준으로 하는 최소 근접 알고리즘
    6. 심층 연결 기반의 신경망
    7. 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블


* **앙상블**: 서로 다른/또는 같은 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로 배깅(Bagging)과 부스팅(Boosting) 방식으로 나눔
    * 배깅 방식의 대표인 랜덤 포레스트는 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등으로 많이 사용하는 알고리즘
    * 부시팅의 효시라고 할 수 있는 그래디언트 부스팅은 뛰어난 예측 성능을 가지지만, 수행 시간이 너무 오래 걸리는 단점으로 인해 최적화 모델 튜닝이 어려웠음. 하지만 XgBoost(eXtra Gradient Boost)와 LightGBM 등 기존 그래디언트 부스팅의 예측 성능을 한 단계 발전시키면서도 수행 시간을 단축시킨 알고리증이 등장
    
    
* 앙상블의 기본 알고리즘으로 일반적으로 사용하는 것이 결정 트리
* 결정트리는 매우 쉽고 유연하게 적용할 수 있는 알고리즘이며 데이터의 스케일링이나 정규화 등의 사전 가공의 영향잉 매우 적음. 하지만 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며, 이로 인한 과적합이 발생해 반대로 예측 성능이 저하될 수도 있음. 이 단점이 앙상블에서는 장점으로 작용
* 앙상블은 매우 많은 여러 개의 약한 학습기(예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시키는데, 결정 트리가 좋은 약한 학습기가 됨

## 2. 결정 트리(Decision Tree)
* 결정 트리: 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것
    * 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함
    
![결정 트리 구조](https://media.vlpt.us/images/dbj2000/post/9899931f-a1f1-4c00-bbbb-89d4834e1105/image.png)

* 결정 트리 구조
    * 규칙 노드(Decision Nonde): 규칙 조건 부분
    * 리프 노드(Leaf Node): 결정된 클래스 값
    * 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됨
    * 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어짐. 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 것이고, 이는 과적합으로 이어지기 쉬움
    * 즉, 트리의 깊이(depth)가 긱ㅍ어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음
    

* 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야함
* 어떻게 트리를 분할(Split)할 것인가가 중요한데 최대한 균일 데이터 세트를 구성할 수 있도록 분할하는 것이 필요


* 데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미침
* 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듦. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측


* 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 **정보 이득(Information Gain) 지수와 지니 계수**가 있음
    * 정보 이득은 엔트로피 개념을 기반으로 함. 엔트로피는 주어진 데이터 집합의 혼잡도를 의미. 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 것이 섞여 있으면 엔트로피가 낮음. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값( 1 - 엔트로피 지수). 결정 트리는 이 정보 이득 지수로 분할 기준을 정하므로 정보 이득이 높은 속성을 기준으로 분할
    * 지니 계수는 경제학에서 불평등 지수를 나타낼 때 사용하는 계수. 0이 가장 평등하고 1로 갈수록 불평등. 머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할

**결정 트리 알고리즘**
* 사이킷런에서 구현한 DecisionTreeClassifier는 기본으로 지니 계쑤를 이용해 데이터 세트를 분할
* 결정 트리ㅢ 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노트에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정

### 2.1 결정 트리 모델의 특징
1. 결정 트리의 가장 큰 장점은 정보의 '균일도'라는 률을 기반으로 하고 있어 알고리즘이 쉽고 직관적이라는 점
    * 결정 트리가 룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고, 시각화로 표현까지 할 수 있음
    * 정보의 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 치퍼의 스케일링과 정규화 같은 전처리 작업이 필요 없음


2. 결정 트리 모델의 가장 큰 단점은 과적합으로 정확도가 떨어진다는 점
    * 피처 정보의 균일도에 따른 룰 규칙으로 서브 트리를 계속 만들다 보면 피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 커지고 복잡해질 수밖에 없음
    * 모든 데이터 상황을 만족하는 완벽한 규칙은 만들지 못하는 경우가 오히려 더 많음에도 불구하고 결정 트리는 학습 데이터 기반 모델의 정확도를 높이기 위해 계속해서 조건을 추가하면서 트리 깊이가 계속 커지고, 결과적으로 복잡해짐
    * 복잡한 모델은 테스트 데이터 세트에 유연하게 대처할 수 없어 예측 성능이 떨어짐
    * 트리의 크기를 사전에 제한하는 것이 성능 튜닝에 도움이 됨

### 2.2 결정 트리 파라미터
* 사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier와 DecisionTreeRegressor 클래스를  제공
    * DecisionTreeClassifier는 분류를 위한 클래스, DecisionTreeRegressor는 회귀를 위한 클래스
* 사이킷런의 결정 트리 구현은 CART(Classification And Regression Trees) 알고리즘 기반
    * CART는 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘
* 여기서는 분류를 위한 DecisionTreeClassifier 클래스만 다룸
* DecisionTreeClassifier와 DecisionTreeRegressor는 동일한 파라미터를 사용

파라미터 명 | 설명
:-----------|:----
min_samples_split| - 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용됨
                 | - 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가
                 | - 과적합을 제어. 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가
min_samples_leaf| - 말단 노드가 되기 위한 최소한의 샘플 데이터 수
                | - min_samples_split와 유사하게 과적합 제어 용도. 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요
max_features| - 최적의 분할을 위해 고려할 최대 피처 개수, 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행
            | - int 혛으로 지정하면 대상 피처의 개수. float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트임
            | - 'sqrt'는 전체 피처 중 sqrt(전체 피처 개수)만큼 선정
            | - 'auto'로 지정하면 'sqrt'와 동일
            | - 'log'는 전체 피처 중 log2(전체 피처 개수) 선정
            | - 'None'은 전체 피처 선정
max_depth| - 트리의 최대 깊이를 규정
         | - 디폴트는 None. None으로 설정하면 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴
         | - 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요
max_leaf_nodes| - 말단 노드의 최대 개수

### 2.3 결정 트리 모델의 시각화(Decision Tree Visualization)
* Graphviz 패키지를 사용하면 결정 트리 알고리즘이 어떻나 규칙을 가지고 트리를 생성하는지 시각적으로 보여줄 수 있음
* Graphviz는 원래 그래프 기반의 dot 파일로 기술된 다양한 이미지를 쉽게 시각화할 수 있는 패키지
* 사이킷런은 Graphviz 패키지와 쉽게 인터페이스할 수 있도록 `export_graphviz()` API를 제공
* 사이킷런의 `export_graphviz()`는 함수 인자로 학습이 완료된 Estimator, 피처의 이름 리스트, 레이블 이름 리스트를 입력하면 학습된 결정 트리 규칙을 실제 트리 형태로 시각화해 보여줌

**Graphviz를 이용해 붓꽃 데이터 세트에 결정 트리를 적용**
* Graphviz를 이용해 붓꽃 데이터 세트에 결정 트리를 적용할 때 어떻게 서브 트리가 구성되고 만들어지는지 시각화해봄

* 사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier를 제공해 결정 트리 모델의 학습과 예측을 수행
* 붓꽃 데이터 세트를 DecisionTreeClassifier를 이용해 학습한 뒤 어떤 형태로 규칙 트리가 만들어지는지 확인


```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target,
                                                       test_size=0.2,  random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
```




    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, presort=False,
                           random_state=156, splitter='best')



* 사이킷런의 트리 모듈은 Graphviz를 이용하기 위해 `export_graphviz()` 함수를 제공
    * `export_graphvia()`는 Graphviz가 읽어 들여서 그래프 형태로 시각화할 수 있는 출력 파일을 생성
    * `export_graphviz()`에 인자로 학습이 완료된 estimator, output 파일 명, 결정 클래스의 명칭, 피처의 명칭을 입력해줌


```python
from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함. 
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names , \
feature_names = iris_data.feature_names, impurity=True, filled=True)
```

* 생성된 출력 파일 'tree.dot'을 Graphviz의 파이썬 래퍼 모듈을 후출해 결정 트리의 규칙을 시각적으로 표현


```python
import graphviz

# 위에서 생성된 tree.dot 파일을 Graphviz 읽어서 Jupyter Notebook상에서 시각화 
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```




![svg](https://github.com/slchoi01/slchoi01.github.io/tree/master/image/pymldg/ch4/output_12_0.svg)



**결과 확인**
* 각 규칙에 띠리 브랜치 노드와 말단 리프 노드가 어떻게 구성되는지 알 수 있게 시각화됨
* 리프 노드는 최종 클래스 값이 결정되는 노드. 리프 노드가 되려면 하나의 클래스 값으로 최종 데이터가 구성되거나 리프 노드가 될 수 있는 하이퍼 파라미터 조건을 충족하면 됨
* 자식 노드가 있는 노드는 브랜치 노드이며 자식 노드를 만들기 위한 분할 규칙 조건을 가지고 있음
* 노드 내에 기술된 지표의 의미
    * petal length(cm) <= 2.45와 같이 피처의 조건이 있는 것은 자식 노드를 만들기 위한 규칙 조건. 이 조건이 없으면 리프 노드
    * gini는 다음의 value = []로 주어진 데이터 분포에서의 지니 계수
    * samples는 현 규칙에 해당하는 데이터 건수
    * value = []는 클래스 값 기반의 데이터 건수. 붓꽃 데이터 세트는 클래스 값으로 0, 1, 2를 가지고 있으며, 0: Setosa, 1: Versicolor, 3: Virginica 품종을 가리킴.
* 색깔이 짙어질수록 지니 계수가 낮고 해당 레이블에 속하는 샘플 데이터가 많다는 의미

**!정리!**
* 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 계속해서 만들어 감. 이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 과적합되는 문제점을 가지게 됨. 결정 트리 알고리즘을 제어하는 대부분 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도
    1. max_depth는 결정 트리의 최대 트리 깊이를 제어
    2. min_samples_split는 자식 규칙 노드를 분할해 만들기 위한 최소한의 샘플 데이터 개수
    3. min_samples_leaf는 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정. 리프 노드가 될 수 있는 조건은 디폴트로 1. 다른 클래스 값이 하나도 없이 단독 클래스로만 돼 있거나 단 한 개의 데이터로 돼 있을 경우 리프 노드가 될 수 있다는 것을 의미
        * min__samples_leaf의 값을 키우면 더 이상 분할하지 않고, 리프 노드가 될 수 있는 조건이 완화됨

* 결정 트리는 균일도에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요한 요건
* 중요한 몇 개의 피처가 명확한 규칙 트리를 만드는데 크게 기여하며, 모델을 좀 더 간결하고 이상치에 강한 모델을 만들 수 있기 때문
* 사이킷런은 결정 트리 알고리즘이 학습을 통해 규칙을 정하는 데 있어 피처의 중요한 역할 지표를 DecisionTreeClassifier 객체의 `feature_importances_` 속성으로 제공
    * `feature_importances_`는 ndarray 형태로 값을 반환하며 피처 순서대로 값이 할당됨
    * 값이 높을수록 해당 피처의 중요도가 높다는 의미

**붓꽃 데이터 세트에서 피처별로 결정 트리 알고리즘에서 중요도 추출**
* 위 예제에서 `fit()`으로 학습된 DecisionTreeClassifier 객체 변수인 `df_clf`에서 `feature_importances_` 속성을 가져와 피처별로 중요도 값을 매칭하고 막대그래프로 표현
    * 여러 피처들 중 `petal_length`가 가장 피처 중요도가 높음을 알 수 있음


```python
import seaborn as sns
import numpy as np
%matplotlib inline

# feature importance 추출 
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))

# feature별 importance 매핑
for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))

# feature importance를 column 별로 시각화 하기 
sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)
```

    Feature importances:
    [0.025 0.    0.555 0.42 ]
    sepal length (cm) : 0.025
    sepal width (cm) : 0.000
    petal length (cm) : 0.555
    petal width (cm) : 0.420
    




    <matplotlib.axes._subplots.AxesSubplot at 0x1f364e29400>




![png](https://github.com/slchoi01/slchoi01.github.io/tree/master/image/pymldg/ch4/output_17_2.png)


### 2.4 결정 트리(Decision TREE) 과적합(Overfitting)
* 결정 트리가 어떻게 학습 데이터를 분할해 예측을 수행하는지와 이로 인한 과적합 문제를 시각화해 알아볼 것

**분류를 위한 데이터 세트를 임의로 만듦**
* 사이킷런은 분류를 위한 테스트용 데이터를 쉽게 만들 수 있도록 `make_classification()` 함수를 제공
    * `make_classification()` 호출 시 반환되는 객체는 피처 데이터 세트와 클래스 레이블 데이터 세트임
* 함수를 이용해 2개의 피처가 4가지 유형의 클래스 값을 가지는 데이터 세트를 만들고 그래프 형태로 시각화


```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline

plt.title("3 Class values with 2 Features Sample data creation")

# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성. 
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,
                             n_classes=3, n_clusters_per_class=1,random_state=0)

# plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨. 
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')
```




    <matplotlib.collections.PathCollection at 0x1f364f85080>




![png](https://github.com/slchoi01/slchoi01.github.io/tree/master/image/pymldg/ch4/output_19_1.png)


* 각 피처가 X, Y축으로 나열된 2차원 그래프이며, 3개의 클래스 값 구분은 색깔로 되어 있음

**결정 트리 학습**
* X_features와 y_labels 데이터 세트를 기반으로 결정 트리를 학습


1. 첫 번째 학습
    * 결정 트리 생성에 별다른 제약이 없도록 결정 트리의 하이퍼 파라미터를 디폴트로 한 뒤, 결정 트리 모델이 어떠한 결정 기준을 가지고 분할하면서 데이터를 분류하는지 확인할 것
    * 별도의 함수 `visualize_boundary()`를 생성. 이 함수는 머신러닝 모델이 클래스 값을 예측하는 결정 기준을 색상과 경계로 나타내 모델이 어떻게 데이터 세트를 예측 분류하는지 이해할 수 있게 해줌
    * `visualize_boudary()`는 유틸리티 함수


```python
import numpy as np

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() 를 이용하여 class boundary 를 visualization 수행. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)
```

* 결정 트리 생성에 별다른 제약이 없도록 하이퍼 파라미터가 디폴트인 Classifier를 학습하고 결정 기준 경계를 시각화


```python
from sklearn.tree import DecisionTreeClassifier

# 특정한 트리 생성 제약없는 결정 트리의 Decsion Boundary 시각화.
dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
```

    C:\Users\chkwon\Anaconda3\lib\site-packages\matplotlib\contour.py:960: UserWarning: The following kwargs were not used by contour: 'clim'
      s)
    


![png](https://github.com/slchoi01/slchoi01.github.io/tree/master/image/pymldg/ch4/output_24_1.png)


**결과**
* 일부 이상치 데이터까지 분류하기 위해 분할이 자주 일어나 결정 기준 경계가 매우 많아짐
* 결정 트리의 기본 하이퍼 파라미터 설정은 리프 노드 안에 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준으로 인해 결정 기준 경계가 많아지고 복잡해짐
* 이렇게 복잡한 모델은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어지게 됨

2. `min_samples_leaf = 6`을 설정
    * 6개 이하의 데이터는 리프 노드를 생성할 수 있도록 리프 노드 생성 규칙을 완화한 뒤 하이퍼 파라미터를 변경해 어떻게 결정 기준 경계가 변하는지 살펴봄


```python
# min_samples_leaf=6 으로 트리 생성 조건을 제약한 Decision Boundary 시각화
dt_clf = DecisionTreeClassifier( min_samples_leaf=6).fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
```

    C:\Users\chkwon\Anaconda3\lib\site-packages\matplotlib\contour.py:960: UserWarning: The following kwargs were not used by contour: 'clim'
      s)
    


![png](https://github.com/slchoi01/slchoi01.github.io/tree/master/image/pymldg/ch4/output_27_1.png)


**결과**
* 이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐음
* 다양한 테스트 데이터 세트를 기반으로 한 결정 트리 모델의 예측 성능은 첫 번째 모델보다는 `min_samples_leaf = 6`으로 트리 생성 조건을 제약한 모델이 더 뛰어날 가능성이 높음
    * 이유: 테스트 데이터 세트는 학습 데이터 세트와는 다른 데이터 세트인데, 학습 데이터에만 지나치게 최적화된 분류 기준은 오히려 테스트 데이터 세트에서 정확도를 떨어뜨릴 수 있기 때문

### 2.5 결정 트리 실습 - Human Activity Recognition
* 결정 트리를 이용해 UCI 머신러닝 리포지토리에서 제공하는 사용자 행동 인식 데이터 세트에 대한 예측 분류를 수행
* 이 데이터는 30명에게 스마트폰 센서를 장착한 뒤 사람의 동작과 관련된 여러 가지 피처를 수집한 데이터
* 수집된 피치 세트를 기반으로 결정 트리를 이용해 어떻나 동작인지 예측을 수행


* `human_activity`의 서브 디렉터리인 train과 test 디렉터리에는 학습 용도의 피처 데이터 세트와 레이블 데이터 세트, 테스트용 피처 데이터 세트와 클래스 값 데이터 세트가 들어 있음
* 피처는 모두 561개가 있으며, 공백으로 분리돼 있음


* 'features.txt' 파일을 사용해 피처 인덱스와 피처명을 살펴봄
    * 피처명을 보면 인체의 움직임과 관련된 속성의 평균/표준편차가 X, Y, Z축 값으로 돼 있음을 유추 가능


```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.
feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])

# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출
feature_name = feature_name_df.iloc[:, 1].values.tolist()
print('전체 피처명에서 10개만 추출:', feature_name[:10])

```

    전체 피처명에서 10개만 추출: ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']
    

* 피처명을 가지는 DataFrame을 이용해 데이터 파일을 데이터 세트 DataFrame에 로딩하기 전 유의해야 할 부분이 있음
    * 피처명을 가지고 있는 features_info.txt 파일은 중복된 피처명을 가지고 있는데 중복된 피처명들을 이용해 데이터 파일을 데이터 세트 DataFrame에 로드하면 오류가 발생. 따라서 중복된 피처병에 대해 원분 피처명에 _1 또는 _2를 추가로 부여해 변경한 뒤 이를 이용해 데이터를 DataFrame에 로드

**중복된 피처명을 확인**


```python
feature_dup_df = feature_name_df.groupby('column_name').count()
print(feature_dup_df[feature_dup_df['column_index'] > 1].count())
feature_dup_df[feature_dup_df['column_index'] > 1].head()
```

    column_index    42
    dtype: int64
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>column_index</th>
    </tr>
    <tr>
      <th>column_name</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>fBodyAcc-bandsEnergy()-1,16</td>
      <td>3</td>
    </tr>
    <tr>
      <td>fBodyAcc-bandsEnergy()-1,24</td>
      <td>3</td>
    </tr>
    <tr>
      <td>fBodyAcc-bandsEnergy()-1,8</td>
      <td>3</td>
    </tr>
    <tr>
      <td>fBodyAcc-bandsEnergy()-17,24</td>
      <td>3</td>
    </tr>
    <tr>
      <td>fBodyAcc-bandsEnergy()-17,32</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>



* 원본 데이터에 중복된 Feature 명으로 인하여 신규 버전의 Pandas에서 Duplicate name 에러를 발생
* 중복 feature명에 대해서 원본 feature 명에 '_1(또는2)'를 추가로 부여하는 함수인 `get_new_feature_name_df()` 생성


```python
def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),
                                  columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) 
                                                                                         if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df
```

**학습/테스트용 DataFrame 로드**
* train 디렉터리에 있는 학습용 피처 데이터 세트와 레이블 데이터 세트, test 디렉터리에 있는 테스트용 피처 데이터 파일과 레이블 데이터 파일을 각각 학습/테스트용 DataFrame에 로드


```python
import pandas as pd

def get_human_dataset( ):
    
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
    
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. 
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./human_activity/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./human_activity/test/X_test.txt',sep='\s+', names=feature_name)
    
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./human_activity/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./human_activity/test/y_test.txt',sep='\s+',header=None,names=['action'])
    
    # 로드된 학습/테스트용 DataFrame을 모두 반환 
    return X_train, X_test, y_train, y_test


X_train, X_test, y_train, y_test = get_human_dataset()
```

* 로드한 학습용 피처 데이터 세트 살펴보기
    * 학습 데이터 세트는 7352개의 레코드로 561개의 피처를 가지고 있음
    * 피처가 전부 float 형의 숫자 형이므로 별도의 카테고리 인코딩은 수행할 필요 없음


```python
print('## 학습 피처 데이터셋 info()')
print(X_train.info())
```

    ## 학습 피처 데이터셋 info()
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 7352 entries, 0 to 7351
    Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean)
    dtypes: float64(561)
    memory usage: 31.5 MB
    None
    

* 레이블 값은 6개 값이고 분포도는 특정 값으로 왜곡되지 않고 비교적 고르게 분포돼 있음


```python
print(y_train['action'].value_counts())
```

    6    1407
    5    1374
    4    1286
    1    1226
    2    1073
    3     986
    Name: action, dtype: int64
    

**동작 얘측 분류 수행**
* 사이킷런이 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행
* DecisionTreeClassifier의 하이퍼 파라미터는 모두 디폴트 값으로 설정해 수행하고, 하이퍼 파라미터 값을 모두 추출
* 결과: 85.48%의 정확도


```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정
dt_clf = DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train , y_train)
pred = dt_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred)
print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))

# DecisionTreeClassifier의 하이퍼 파라미터 추출
print('DecisionTreeClassifier 기본 하이퍼 파라미터:\n', dt_clf.get_params())
```

    결정 트리 예측 정확도: 0.8548
    DecisionTreeClassifier 기본 하이퍼 파라미터:
     {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': 156, 'splitter': 'best'}
    

**트리 깊이가 예측 정확도에 영향을 주는지 확인**
* 결정 트리의 경우 분류를 위해 리프 노드가 될 수 있는 적합한 수준이 될 때까지 지속해서 트리의 분할을 수행하면서 깊이가 깊어짐
* GridSearchCV를 이용해 사이킷런 결정 트리의 깊이를 조절할 수 있는 하이퍼 파라미터인 `max_depth` 값을 변화시키면서 예측 성능을 확인
* 결과: `max_depth`가 8일 때 5개의 폴드 세트의 최고 평균 정확도 결과가 약 85.26%로 도출됨


```python
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth' : [ 6, 8 ,10, 12, 16 ,20, 24]
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )
grid_cv.fit(X_train , y_train)
print('GridSearchCV 최고 평균 정확도 수치:{0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)

```

    Fitting 5 folds for each of 7 candidates, totalling 35 fits
    

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  1.7min finished
    

    GridSearchCV 최고 평균 정확도 수치:0.8526
    GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 8}
    

* 5개의 CV 세트에서 `max_depth` 값에 따라 어떻게 예측 성능이 변했는지 GridSearchCV 객체의 `cv_results_` 속성을 통해 살펴봄
    * `cv_results_` 속성은 CV세트에 하이퍼 파라미터를 순차적으로 입력했을 때의 성능 수치를 가지고 있음
    * `mean_test_score`는 5개 CV 세트에서 검증용 데이터 세트의 정확도 평균 수치임


```python
# GridSearchCV객체의 cv_results_ 속성을 DataFrame으로 생성. 
cv_results_df = pd.DataFrame(grid_cv.cv_results_)

# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출
cv_results_df[['param_max_depth', 'mean_test_score']]

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>param_max_depth</th>
      <th>mean_test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>6</td>
      <td>0.850925</td>
    </tr>
    <tr>
      <td>1</td>
      <td>8</td>
      <td>0.852557</td>
    </tr>
    <tr>
      <td>2</td>
      <td>10</td>
      <td>0.850925</td>
    </tr>
    <tr>
      <td>3</td>
      <td>12</td>
      <td>0.844124</td>
    </tr>
    <tr>
      <td>4</td>
      <td>16</td>
      <td>0.852149</td>
    </tr>
    <tr>
      <td>5</td>
      <td>20</td>
      <td>0.851605</td>
    </tr>
    <tr>
      <td>6</td>
      <td>24</td>
      <td>0.850245</td>
    </tr>
  </tbody>
</table>
</div>



**별도의 테스트 데이터 세트에서 결정 트리의 정확도를 측정**
* 별도의 테스트 데이터 세트에서 `max_depth`의 변화에 따른 값을 측정


```python
max_depths = [ 6, 8 ,10, 12, 16 ,20, 24]
# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정
for depth in max_depths:
    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)
    dt_clf.fit(X_train , y_train)
    pred = dt_clf.predict(X_test)
    accuracy = accuracy_score(y_test , pred)
    print('max_depth = {0} 정확도: {1:.4f}'.format(depth , accuracy))
```

    max_depth = 6 정확도: 0.8558
    max_depth = 8 정확도: 0.8707
    max_depth = 10 정확도: 0.8673
    max_depth = 12 정확도: 0.8646
    max_depth = 16 정확도: 0.8575
    max_depth = 20 정확도: 0.8548
    max_depth = 24 정확도: 0.8548
    

**결과**
* 앞의 GridSearchCV 예제와 마찬가지로 깊이가 깊어질수록 테스트 데이터 세트의 정확도는 더 떨어짐
* 결정 트리는 깊이가 깊어질수록 과적합의 영향력이 커지므로 하이퍼 파라미터를 이용해 깊이를 제어할 수 있어야 함
* 복잡한 모델보다도 트리 깊이를 낮춘 단순한 모델이 효과적인 결과를 가져올 수 있음

**정확도 성능을 튜닝**
* `max_depth`와 `min_samples_split`으 같이 변경하면서 정확도 성능을 튜닝
    * 결과: `max_depth`가 8, `min_samples_split`이 16일 때 가장 최고의 정확도로 약 85.5%를 나타냄


```python
params = {
    'max_depth' : [ 8 , 12, 16 ,20], 
    'min_samples_split' : [16,24],
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )
grid_cv.fit(X_train , y_train)
print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)

```

    Fitting 5 folds for each of 8 candidates, totalling 40 fits
    

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.1min finished
    

    GridSearchCV 최고 평균 정확도 수치: 0.8550
    GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 8, 'min_samples_split': 16}
    

**별도 분리된 테스트 데이터 세트에 해당 하이퍼 파라미터를 적용**
* GridSearchCV 객체인 `grid_cv`의 속성인 `best_estimator_`는 최적 하이퍼 파라미터인 max_depth 8, min_samples_split 16으로 학습이 완료된 Estimator 객체
* 이 객체를 이용해 테스트 데이터 세트에 예측을 수행



```python
best_df_clf = grid_cv.best_estimator_
pred1 = best_df_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred1)
print('결정 트리 예측 정확도:{0:.4f}'.format(accuracy))
```

    결정 트리 예측 정확도:0.8717
    

**결정 트리에서 각 피치의 중요도 확인**
* `feature_importances_` 속성을 이용해 알아봄. 중요도가 높은 순으로 Top 20 피처를 막대그래프로 표현


```python
import seaborn as sns

ftr_importances_values = best_df_clf.feature_importances_
# Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns  )
# 중요도값 순으로 Series를 정렬
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]
plt.figure(figsize=(8,6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20 , y = ftr_top20.index)
plt.show()
```


![png](https://github.com/slchoi01/slchoi01.github.io/tree/master/image/pymldg/ch4/output_56_0.png)


* 막대 그래프상에서 가장 중요도를 가진 Top 5의 피처들이 매우 중요하게 규칙 생성에 영향을 미치고 있는 것을 알 수 있음
