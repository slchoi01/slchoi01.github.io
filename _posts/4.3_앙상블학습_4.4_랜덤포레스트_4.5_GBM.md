
## 1. 앙상블 학습 개요
* 앙상블 학습을 통한 분류는 여러 개의 분류기를 생성하고 그 예측을 결합함으로써 정확한 최종 예측을 도출하는 기법을 의미
* 앙상블 학습의 목표는 다양한 분류기의 예측 결과를 결합함으로써 단일 분류기보다 신뢰성이 높은 예측값을 얻는 것
* 이미지, 영상, 음성 등의 비정형 데이터의 분류는 딥러닝이 뛰어난 성능을 보이고 있지만, 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있음


* 앙상블 학습의 유형은 전통적으로 보팅(Voting), 배깅(Bagging), 부스팅(Boosting) 세 가지로 나눌 수 있으며, 이외에도 스태깅을 포함한 다양한 앙상블 방법이 있음
    * 보팅과 배깅은 여러 개의 분류기가 토표를 통해 최종 예측 결과를 결정하는 방식
    * 보팅과 배깅의 다른 점은 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합하는 것이고, 배깅의 경우 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것
    * 대표적인 배깅 방식이 랜텀 포레스트 알고리즘
    
![Voting 방식과 Bagging 방식](https://media.vlpt.us/images/kjpark4321/post/c5208df8-0c2e-48b6-a727-b8996633d167/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-01-14%20%EC%98%A4%EC%A0%84%201.02.01.png)
* 왼쪽 그림은 보팅 분류기를 도식화한 것. 선형 회귀, K 최근접 이웃, 서포트 벡터 머신이라는 3개의 ML 알고리즘이 같은 데이터 세트에 대해 학습하고 예측한 결과를 가지고 보팅을 통해 최종 예측 결과를 선정하는 방식
* 오른쪽 그램은 배깅 분류기를 도식화한 것. 단일 ML 알고리즘(결정 트리)으로 여러 분류기가 학습으로 개별 예측을 하는데, 학습하는 데이터 세트가 보팅 방식과 다름
    * 개별 분류기에 할당된 학습 데이터는 원본 학습 데이터를 샘플링해 추출하는데, 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 부트스트래핑(Bootstrapping) 분할 방식이라고 부름
    * 개별 분류기가 부트 스트래핑 방식으로 샘플링된 데이터 세트에 대해 학습을 통해 개별적인 예측을 수행한 결과를 보팅을 통해 최종 예측 결과를 선정하는 방식이 바로 배깅 앙상블 방식
    * 교차 검증이 데이터 세트 간에 중첨을 허용하지 않는 것과 다르게 배깅 방식은 중첩을 허용. 따라서 10000개의 데이터를 10개의 분류기가 배깅 방식으로 나누더라도 각 1000개의 데이터 내에는 중복된 데이터가 있음


* 부스팅은 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 부여하면서 학습과 예측을 진행하는 것
* 계속해서 분류기에게 가중치를 부스팅하면서 학습을 진행하기에 부스팅 방식으로 불림
* 예측 성능이 뛰어나 앙상블 학습을 주도하고 있으며 대표적인 부스팅 모듈로 그래디언트 부스트, XGBoost, LightGBM이 있음
* 스태킹은 여러 가지 다른 모델의 예측 결괏값을 다시 학습 데이터로 만들어서 다른 모델로 재학습시켜 결과를 예측하는 방법

### 1.1 보팅 유형 - 하드 보팅(Hard Voting)과 소프트 보팅(Soft Voting)
* 하드 보팅을 이용한 분류기는 다수결 원칙과 비슷. 예측한 결괏값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 것
* 소프트 보팅은 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로 선정
* 일반적으로 소프트 보팅이 보팅 방법으로 적용됨


![하드 보팅 vs 소프트 보팅](https://media.vlpt.us/images/kjpark4321/post/bff303e6-0e82-4066-9032-bad1c0239a21/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-01-14%20%EC%98%A4%EC%A0%84%201.11.58.png)
* 왼쪽 그림은 하트 보팅. Classifier 1번, 2번, 3번, 4번인 4개로 구성한 보팅 앙상블 기법에서 분류기 1번, 3번 ,4번이 1로 레이블 값을 예측하고 분류기 2번이 2로 레이블 값을 예측하면 다수결 원칙에 따라 최종 예측은 레이블 값 1이 됨
* 오룬쪽 그림은 소프트 보팅. 소프트 보팅은 각 분류기의 레이블 값 예측 확률을 평균 내어 푀종 결정. 가령 분류기 1번의 레이블 값 1과 2의 예측 확률이 각각 0.7/0.3이고 분류기 2번은 0.2/0.8, 분류기 3번은 0.8/0.2, 분류기 4번은 0.9/0.1이라면 레이블 값 1의 평균 예측 확률은 분류기 1번, 2번, 3번, 4번의 확률을 모두 더하여 평균하면 0.65가 됨, 레이블 값 2의 평균 예측 확률도 같은 방법으로 계산하면 0.35가 됨. 따라서 레이블 값 1의 확률이 0.65, 레이블 값 2인 확률 0.35보다 크므로 레이블 값 1로 최종 보팅하는 것이 스프트 보팅
* 일반적으로 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아 더 많이 사용됨

### 1.2 보팅 분류기(Voting Classifier)
* 사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스를 제공
* 보팅 방식의 앙상블을 이용해 위스콘신 유방암 데이터 세트르 예측 분석함
    * 위스콘신 유방암 데이터 세트는 유방암의 악성 종양, 양성종양 여부를 결정하는 이진 분류 데이터 세트이며 종양의 크기, 모양 등의 형태와 관련한 많은 피처를 가지고 있음
    * 사이킷런은 `loda_breast_cancer()` 함수를 통해 자체에서 위스콘신 유방암 데이터 세트를 생성할 수 있음


**로지스틱 회귀와 KNN을 기반으로 보팅 분류기 생성**
* 필요한 모듈과 데이터를 로딩한 후 위스콘신 데이터 세트 살펴보기


```python
import pandas as pd

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

cancer = load_breast_cancer()

data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
data_df.head(3)


```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.8</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>25.38</td>
      <td>17.33</td>
      <td>184.6</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.9</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>24.99</td>
      <td>23.41</td>
      <td>158.8</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.0</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>23.57</td>
      <td>25.53</td>
      <td>152.5</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 30 columns</p>
</div>



* 로지스틱 회귀와 KNN을 기반으로 하여 소프트 보팅 방식으로 새롭게 보팅 분류기 생성
    * 사이킷런은 VotingClassifier 클래스를 이용해 보팅 분류기를 생성할 수 있음
    * VotingClassifier 클래스는 주요 생성 인자로 `estimators`와 `voting` 값을 입력 받음
    * `estimators`는 리스트 값으로 보팅에 상요될 여러 개의 Classifier 객체들을 튜플 형식으로 입력 받으며 `voting`은 `hard`시 하드 보팅, `soft`시 소프트 보팅 방식을 적용하라는 의미(디폴트는 `hard`)



```python
# 개별 모델은 로지스틱 회귀와 KNN 임. 
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

# 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 
vo_clf = VotingClassifier( estimators=[('LR',lr_clf),('KNN',knn_clf)] , voting='soft' )

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, 
                                                    test_size=0.2 , random_state= 156)

# VotingClassifier 학습/예측/평가. 
vo_clf.fit(X_train , y_train)
pred = vo_clf.predict(X_test)
print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))

# 개별 모델의 학습/예측/평가.
classifiers = [lr_clf, knn_clf]
for classifier in classifiers:
    classifier.fit(X_train , y_train)
    pred = classifier.predict(X_test)
    class_name= classifier.__class__.__name__
    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test , pred)))
```

    Voting 분류기 정확도: 0.9561
    LogisticRegression 정확도: 0.9474
    KNeighborsClassifier 정확도: 0.9386
    

    C:\Users\slc\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
      FutureWarning)
    C:\Users\slc\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
      FutureWarning)
    

**결과**
* 보팅 분류기가 정확도가 조금 높게 나타남. 보팅으로 여러 개의 기반 분류기를 결합한다고 해서 무조건 기반 분류기보다 예측 성능이 향상되지는 않음. 데이터의 특성과 분포 등 다양한 요건에 따라 오히려 기반 분류기 중 가장 좋은 분류기의 성능이 보팅했을 때보다 나을 수도 있음


**정리**
* 보팅을 포함해 배깅과 부스팅 등의 앙상블 방법은 전반적으로 다른 단일 ML 알고맂므보다 뛰어난 예측 성능을 가지는 경우가 많음
* ML 모델의 성능은 다양한 테스트 데이터에 의해 검증되므로 어떻게 높은 유연성을 가지고 현실에 대처할 수 있는가가 중요한 ML 모델의 평가요소가 됨
* 이런 관점에서 편향-분산 트레이드오프는 ML 모델이 극복해야할 중요 과제
* 보팅과 스태킹 등은 서로 다른 알고리즘을 기반으로 하고 있지만, 배깅과 부스팅은 대부분 결정 트리 알고리즘을 기반으로 함.
    * 결정 트리 알고리즘은 쉽고 직관적인 분류 기준을 가지고 있지만 정확한 예측을 위해 학습 데이터의 예외 사오항에 집학한 나머지 오히려 과적합이 발생해 실제 테스트 데이터에서 예측 성능이 떨어지는 현상이 발생하기 쉬움
    * 앙상블 학습에서는 이 같은 결정 트리 알고리즘의 단점을 수십 ~ 수전 개의 매우 많은 분류기를 결합해 다양한 상황을 학습하게 함으로써 극복하고 있음
    * 결정 트리 알고리즘의 장점은 그대로 취하고 단접은 보완하면서 편향-분산 트레이드오프의 효과를 극대화할 수 있다는 것

## 2. 랜덤 포레스트(Random Forest)

### 2.1 랜덤 포레스트의 개요 및 실습
* 배깅은 보팅과 다르게, 같은 알고리즘으로 여러 개의 분류기를 만들어 보팅으로 최종 결정하는 알고리즘. 배깅의 대표적읜 알고리즘은 랜덤 포레스트
* 랜덤 포레스트는 앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있으며, 다양한 영역에서 높은 예측 성능을 보임
* 랜덤 포레스트의 기반 알고리즘은 결정 트리로서, 결정 트리의 쉽고 직관적인 장점을 그대로 가지고 있음


* 랜덤 포레스트는 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 하게 됨

![랜덤 포레스트](https://media.vlpt.us/images/kjpark4321/post/f0f5c31d-c4c9-4e2a-8bb4-53548aad5b77/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-01-14%20%EC%98%A4%EC%A0%84%201.53.55.png)

* 랜덤 포레스트는 개별적인 분류기의 기반 알고리즘은 결정 트리지만 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가중첩되게 샘플링된 데이터 세트
* 여러 개의 데이터 세트를 중첩되게 분리하는 것을 부트스트래핑(bootstrapping) 분할 방식이라고 함
    * 랜덤 포레스트의 서브세트 데이터는 부트스트래핑으로 데이터가 임의로 만들어짐
    * 서브세트의 데이터 건수는 전체 데이터 건수와 동일하지만, 개별 데이터가 중첩되어 만들어짐
* 원본 데이터의 건수가 10개인 학습 데이터 세트에 랜덤 포레스트를 3개의 결정 트리 기반으로 학습하려고 `n_estimators = 3`으로 하이퍼 파라미터를 부여햐면 아래 그림과 같이 데이터 서브세트가 만들어짐

![부트스트래핑 샘플링 방식](https://media.vlpt.us/images/kjpark4321/post/5c47476a-7abf-45b8-a5ba-b442305933d8/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-01-14%20%EC%98%A4%EC%A0%84%201.57.25.png)

* 데이터가 중첩된 개별 데이터 세트에 결정 트리 분류기를 각각 적용하는 것이 랜덤 포레스트

**사용자 행동 인식 데이터 세트를 랜덤 포레스트를 이용해 예측**
* 사이킷런은 RandomForestClassifier를 통해 랜덤 포레스트 기반의 분류를 지원


* 사용자 행동 데이터 세트에 DataFrame을 반환하는 `get_human_dataset()`를 이용해 학습/테스트용 DataFrame을 가져옴


```python
import pandas as pd

def get_human_dataset( ):
    
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = feature_name_df.iloc[:, 1].values.tolist()
    
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./human_activity/train/X_train.txt',sep='\s+', names=feature_name)
    X_test = pd.read_csv('./human_activity/test/X_test.txt',sep='\s+', names=feature_name)
    
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./human_activity/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./human_activity/test/y_test.txt',sep='\s+',header=None,names=['action'])
    
    # 로드된 학습/테스트용 DataFrame을 모두 반환 
    return X_train, X_test, y_train, y_test


X_train, X_test, y_train, y_test = get_human_dataset()
```

    C:\Users\slc\Anaconda3\lib\site-packages\pandas\io\parsers.py:702: UserWarning: Duplicate names specified. This will raise an error in the future.
      return _read(filepath_or_buffer, kwds)
    


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환
X_train, X_test, y_train, y_test = get_human_dataset()

# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가
rf_clf = RandomForestClassifier(random_state=0)
rf_clf.fit(X_train , y_train)
pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred)
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))
```

    랜덤 포레스트 정확도: 0.9108
    

* 랜덤 포레스트는 사용자 행동 인식 데이터 세트에 대해 약 91.08%의 정확도를 보여줌

### 2.2 랜덤 포레스트 하이퍼 파라미터 및 튜닝
* 트리 기반의 앙상블 알고리즘의 단점은 하이퍼 파라미터가 너무 낳고, 그로 인해 튜닝을 위한 시간이 많이 소모된다는 것. 믾은 시간을 소모했음에도 튜닝 후 예측 성능이 크개 향상되는 경우가 많지 않음
* 트리 기반 자체의 하이퍼 파라미터가 원래 많은 데다 배깅, 부스팅, 학습, 정규화 등을 위하나 하이퍼 파라미터까지 추가되므로 일반적인 다른 ML 알고리즘에 비해 많을 수 밖에 없음
* 랜덤 포레스트는 적은 편에 속한는데, 결정 트리에서 사용되는 하이퍼 파라미터와 같은 파라미터가 대부분이기 때문
* **랜덤 포레스트 하이퍼 파라미터**
    * `n_estimators`: 랜덤 포레스트에서 결정 트리의 개수를 지정. 디폴트는 10개. 맣이 설정할수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성증이 무조건 향상되는 것ㅅ은 아님. 또한 늘릴수록 학습 수행 시간이 오래 걸림
    * `max_features`: 결정 트리에 사용된 `max_features` 파라미터와 같음. RandomForestClassifier의 디폴트는 `None`이 아닌 `auto`(`sqrt`). 따라서 랜덤 포레스트의 트리를 분할하는 피처를 참조할 때 전체 피처가 아니라 sqrt만큼 참조(전체 피처가 16개라면 분할을 위해 4개 참조)
    * `max_depth`나 `min_samples_leaf`와 같이 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터가 랜덤 포레스트에도 똑같이 적용 가능

**GridSearchCV를 이용해 랜덤 포레스트의 하이퍼 파라미터 튜닝**
* 앞의 사용자 행동 데이터 세트를 그대로 이용하고, 튜닝 시간 절약을 위해 `n_estimators`는 100으로, CV를 2로만 설정해 최적 하이퍼 파라미터를 구할 것
* 다른 하이처 파라미터를 최적화한 뒤 `n_estimarots`는 낮우에 300으로 증가시켜 예측 성능을 평가할 것
* 멀티 코어 환경에서는 다음 예제에서 RandomForestClassifier 생성자와 GridSearchCV 생성 시 `n_jobs = -1` 파라미터를 추가하면 모든 CPU 코어를 이용해 학습할 수 있음

* `n_estimarots`가 100, `max_depth: 10`, `min_samples_leaf: 8`, `min_samples_split: 8`일 때 약 91.66%의 평균 정확도가 측정됨


```python
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators':[100],
    'max_depth' : [6, 8, 10, 12], 
    'min_samples_leaf' : [8, 12, 18 ],
    'min_samples_split' : [8, 16, 20]
}
# RandomForestClassifier 객체 생성 후 GridSearchCV 수행
rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)
grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )
grid_cv.fit(X_train , y_train)

print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```

    최적 하이퍼 파라미터:
     {'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 100}
    최고 예측 정확도: 0.9166
    

* `n_estimators`를 300으로 증가시키고, 최적화 하이퍼 파라미터로 다시 RandomForestClassifier를 학습시킨 뒤 별도의 테스트 데이터에서 예측 성능을 측정


```python
rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, \
                                 min_samples_split=8, random_state=0)
rf_clf1.fit(X_train , y_train)
pred = rf_clf1.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))
```

    예측 정확도: 0.9165
    

* 결과: 약 91.65%


* RandomForestClassifier도 `feature_importances_` 속성을 잉요해 알고리즘이 선택한 피처의 중요도를 알 수 있음. 피처 중요도를 막대그래프로 시각화


```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

ftr_importances_values = rf_clf1.feature_importances_
ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns  )
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]

plt.figure(figsize=(8,6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20 , y = ftr_top20.index)
plt.show()
```


![png](output_19_0.png)


* tGravityAcc-min()-X, tGravityAcc-mean()-Y, tGravityAcc-min()-Y 등이 높은 피처 중요도를 가지고 있음

## 3. GBM(Gradient Boosting Machine)

### 3.1 GBM의 개요 및 실습
* 부스팉 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부텨를 통해 오류를 개선해 나가면서 학습하는 방식
* 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)과 그래디언트 부스트가 있음


**1. 에이다 부스트**
* 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘

![에이다 부스트](https://media.vlpt.us/images/kjpark4321/post/4bdb3c8d-0fc0-42de-ae9c-206baa7534f3/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-01-14%20%EC%98%A4%EC%A0%84%202.02.47.png)
* 맨 왼쪽 그림과 같이 +와 -로 된 피처 데이터 세트가 있다면
    * Step 1은 첫 번째 약한 학습기가 분류 기준 1로 +와 -를 분류한 것. 동그라미로 표시된 + 데이터는 + 데이터가 잘못 분류된 오류 데이터
    * Step 2에서는 이 오류 데이터에 대해 가중치 값을 부여. 가중치가 부여된 오류 + 데이터는 다음 약한 학습기가 더 잘 분류할 수 있게 크기가 커짐
    * Step 3은 두 번째 약한 학습기가 분류 기준 2로 +와 -를 분류. 마찬가지로 동그라미로 표시된 - 데이터는 잘못 분류된 오류 데이터
    * Step 4에서는 잘못 분류된 이 - 오류 데이터에 대해 다음 약한 학습기ㄱ가 잘 분류할 수 있게 더 큰 가중치를 부여
    * Step 5는 세번째 약한 학습기가 분류 기준 3으로 +와 -를 분류하고 오류 데이터를 찾음. 에이다부스트는 이렇게 약한 학습기가 순차적으로 오류 값에 대해 가중치를 부여한 예측 결정 기준을 모두 결합해 예측을 수행
    * 마지막으로 맨 아래에는 첫 번째, 두 번째, 세 번째 약한 학습기를 몯 결합한 결과 예측. 개별 약한 학습기보다 훨씬 정확도가 높아졌음을 알 수 있음
    
    
* 개별 약한 학습기는 각각 가중치를 부여해 결합. 첫 번째 학습기에 가중치 0.3, 두 번째 학습기에 가중치 0.5, 세 번째 학습기에 가중치 0.8을 부여한 후 모두 결합해 예측을 수행
    
    

**2. GBM(Gradient Boost Machine)**
* 에이다부스트와 유사하나, 가중치 업데이터를 경사 하강법을 이용하는 것이 큰 차이. 오류 값은 실제 값 - 예측값임
* 분류의 실제 결과값을 $y$, 피처를 $x_1, x_2, ..., x_n$, 그리고 이 피처에 기반한 예측 함수를 $F(x)$ 함수라고 하면 오류 식 $h(x) = y - F(x)$이 됨. 이 오류식을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트하는 것이 경사 하강법

* GBM은 CART 기반의 다른 알고리즘과 마찬가지로 분류와 회귀 모두 가능
* 사이킷런은 GBM 기반의 분류를 위해 GradientBoostingClassifier 클래스를 제공

**GBM을 이용해 사용자 행동 데이터 세트 예측 분류**
* `get_human_dataset()` 함수로 데이터 세트를 가져옴


```python
from sklearn.ensemble import GradientBoostingClassifier
import time
import warnings
warnings.filterwarnings('ignore')

X_train, X_test, y_train, y_test = get_human_dataset()

# GBM 수행 시간 측정을 위함. 시작 시간 설정.
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train , y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print("GBM 수행 시간: {0:.1f} 초 ".format(time.time() - start_time))

```

    GBM 정확도: 0.9386
    GBM 수행 시간: 400.8 초 
    

* 결과: 기본 하이퍼 파라미터만으로 93.76%의 예측 정확도로 앞의 랜덤 포레스트보다 나은 예측 성능을 나타냄


**정리**
* 일반적으로 GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많음. 그러나 수행 시간이 오래 결리고, 하이퍼 파라미터 튜닝 노력도 더 필요
* 사이킷런의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬 처리가 지원되지 않아 대용량 데이터의 경우 학습에 매우 많은 시간이 필요. 반면 랜텀 포레스트의 경우 상대적으로 빠른 수행 시간을 보장해주기 때문에 더 쉽게 예측 결과를 도출할 수 있음

### 3.2 GBM 하이퍼 파라미터 및 튜닝
* GBM 하이퍼 파라미터
    * `n_estimators`, `max_depth`, `max_features` 와 같은 트리 가반 자체의 파라미터는 소개 생략
    * `loss`: 경사 하강법에서 사용할 비용 함수를 지정. 특별한 이유가 없으면 기본값인 `deviance`를 그대로 적용
    * `learning_rate`: GBM이 학습을 진행할 대마다 적용하는 학습률. Weak learner가 순차적으로 오류 값을 보정해 나가는데 적용하는 계수. 0~1 사이의 값을 지정할 수 있으며 기본값은 0.1. 너무 작은 값을 적용하면 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높음. 하지만 많은 weak learner는 순차적인 반복이 필요해서 수행 시간이 오래 걸리고, 또 너무 작게 설정하면 모든 waek learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있음. 반대로 너무 큰 값을 적용하면 최소 오류 값을 찾지 못하고 그냥 지나쳐 버려 예측 성능이 떨어질 가증성이 높아지지만, 빠른 수행이 가능. 이런 특성 때문에 `learning_rate`는 `n_estimators`와 상호 보완적으로 조합해 사용. `learning_rate`를 작게 하고 `n_estimators`를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측 성능이 조금씩 좋아질 수 있음. 하지만 수행 시간이 너무 오래 걸리는 단점이 있으며, 예측 성능 역시 현격히 좋아지지는 않음
    * `n_estimators`: weak learner의 개수. weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있음. 하지만 개수가 많을수록 수행 시간이 오래 걸림. 기본값은 100.
    * `subsample`: weak learner가 학습에 사용하는 데이터의 샘플링 비율. 기본값은 1이며, 전체 학습 데이터를 기반으로 학습한다는 의미. 과적합이 염려되는 경우 subsample을 1보다 작은 값으로 설정
    

**GridSearchCV를 이용해 하이퍼 파라미터 최적화 수행**
* 사용자 행동 데이터 세트 정도의 데이터 양에 많은 하이퍼 파라미터를 GBM으로 테스트하려면 시간이 많이 걸리므로 간략하게 `n_estimators`를 100, 500으로 `learning_rate`를 0.05, 0.1로만 제약. 그리고 교차 검증 세트도 2개로만 설정해 GridSearchCV를 적용


```python
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators':[100, 500],
    'learning_rate' : [ 0.05, 0.1]
}
grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1)
grid_cv.fit(X_train , y_train)
print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```

    Fitting 2 folds for each of 4 candidates, totalling 8 fits
    

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 45.7min finished
    

    최적 하이퍼 파라미터:
     {'learning_rate': 0.05, 'n_estimators': 500}
    최고 예측 정확도: 0.9014
    

* 결과: `learning_rate`가 0.05, `n_estimators`가 500일 때 2개의 교차 검증 세트에서 90.1% 정확도가 최고로 도출됨



**테스트 데이터 세트에서 예측 정확도 확인**
* 위 설정을 그대로 테스트 데이터 세트에 적용해 예측 정확도 확인


```python
# GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. 
gb_pred = grid_cv.best_estimator_.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
```

    GBM 정확도: 0.9396
    

* 결과: 약 93.96%

**정리**
* GBM은 과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘. 하지만 수행 시간이 오래 걸린다는 단점이 있음


```python

```
